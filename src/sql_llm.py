from langchain_groq import ChatGroq

from sqlalchemy import create_engine
from sqlalchemy.pool import StaticPool
from langchain_community.utilities.sql_database import SQLDatabase
from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit

from langchain_community.tools.sql_database.tool import (
    InfoSQLDatabaseTool,
    ListSQLDatabaseTool,
    QuerySQLCheckerTool,
    QuerySQLDataBaseTool,
)
from langchain import hub
from langgraph.prebuilt import create_react_agent

from dotenv import load_dotenv

import os



load_dotenv()

# import api key from groq  
api_key = os.getenv('GROQ_API_KEY')

llm = ChatGroq(
    api_key=api_key,
    model_name='llama-3.2-90b-vision-preview'
    )


prompt_template = hub.pull("langchain-ai/sql-agent-system-prompt")

system_message = prompt_template.format(dialect="SQLite", top_k=5)

class SqlLlm:
    """ Integrate a SQLite database with a large language model (LLM) to process natural language queries and generate intelligent responses. """    

    def __init__(self, conn, llm=llm):
        """Initialize the SqlLlm class.

        Args:
            conn: A connection to an existing SQLite database.
            llm: A large language model (LLM) instance to handle natural language interactions.
        """
        self.engine = create_engine(
            "sqlite://",  # Use SQLite as the database backend
            creator=lambda: conn,  # Use the provided connection as the source
            poolclass=StaticPool,  # Avoid connection pooling (use a static connection)
            connect_args={"check_same_thread": False}  # Allow multithreaded access
        )
        # Create a SQLDatabase wrapper around the engine for database interactions
        self.db = SQLDatabase(self.engine)
        # Create a toolkit that links the database and the LLM for generating SQL queries
        self.toolkit = SQLDatabaseToolkit(db=self.db, llm=llm)
        # Create a reactive agent that can use the tools provided by the toolkit
        self.agent_executor = create_react_agent(
            llm,  # The language model
            self.toolkit.get_tools(),  # The tools (e.g., SQL execution)
            state_modifier=system_message  # Initial state customizations for the agent
        )

    def _create_answer(self, query: str) -> str:
        """Process a query and return the generated response.

        Args:
            query: A string containing the user's query in natural language or specific format.

        Returns:
            answer: A string containing the response generated by the LLM after interacting with the database.
        """
        # Execute the query using the agent in streaming mode
        events = self.agent_executor.stream(
            {"messages": [("user", query)]},  # Pass the user's query in the expected format
            stream_mode="values"  # Stream results as they are generated
        )

        # Collect and process the events/messages from the agent
        events_messages = []
        for event in events:
            # event["messages"][-1].pretty_print()  # Print the most recent message for debugging
            events_messages.append(event["messages"][-1])  # Collect the last message in the list
            # print(f"event: {event}")
        
        answer = events_messages[-1].content
        # Return the content of the last message in the sequence, which is the llm answer
        return answer